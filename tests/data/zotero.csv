"Key","Item Type","Publication Year","Author","Title","Publication Title","ISBN","ISSN","DOI","Url","Abstract Note","Date","Date Added","Date Modified","Access Date","Pages","Num Pages","Issue","Volume","Number Of Volumes","Journal Abbreviation","Short Title","Series","Series Number","Series Text","Series Title","Publisher","Place","Language","Rights","Type","Archive","Archive Location","Library Catalog","Call Number","Extra","Notes","File Attachments","Link Attachments","Manual Tags","Automatic Tags","Editor","Series Editor","Translator","Contributor","Attorney Agent","Book Author","Cast Member","Commenter","Composer","Cosponsor","Counsel","Interviewer","Producer","Recipient","Reviewed Author","Scriptwriter","Words By","Guest","Number","Edition","Running Time","Scale","Medium","Artwork Size","Filing Date","Application Number","Assignee","Issuing Authority","Country","Meeting Name","Conference Name","Court","References","Reporter","Legal Status","Priority Numbers","Programming Language","Version","System","Code","Code Number","Section","Session","Committee","History","Legislative Body"
"Q5BGEGIZ","preprint","2022","Kan, Kelvin; Aubet, François-Xavier; Januschowski, Tim; Park, Youngsuk; Benidis, Konstantinos; Ruthotto, Lars; Gasthaus, Jan","Multivariate Quantile Function Forecaster","","","","","http://arxiv.org/abs/2202.11316","We propose Multivariate Quantile Function Forecaster (MQF2), a global probabilistic forecasting method constructed using a multivariate quantile function and investigate its application to multi-horizon forecasting. Prior approaches are either autoregressive, implicitly capturing the dependency structure across time but exhibiting error accumulation with increasing forecast horizons, or multi-horizon sequence-to-sequence models, which do not exhibit error accumulation, but also do typically not model the dependency structure across time steps. MQF2 combines the beneﬁts of both approaches, by directly making predictions in the form of a multivariate quantile function, deﬁned as the gradient of a convex function which we parametrize using inputconvex neural networks. By design, the quantile function is monotone with respect to the input quantile levels and hence avoids quantile crossing. We provide two options to train MQF2: with energy score or with maximum likelihood. Experimental results on real-world and synthetic datasets show that our model has comparable performance with state-ofthe-art methods in terms of single time step metrics while capturing the time dependency structure.","2022-02-23","2022-05-17 21:30:51","2022-05-17 21:30:51","2022-05-17 21:30:51","","","","","","","","","","","","arXiv","","en","","","","","arXiv.org","","Number: arXiv:2202.11316 arXiv:2202.11316 [cs, stat]","","/home/rdyro/Zotero/storage/D8AE7R67/Kan et al. - 2022 - Multivariate Quantile Function Forecaster.pdf","","","Computer Science - Machine Learning; Statistics - Machine Learning","","","","","","","","","","","","","","","","","","","arXiv:2202.11316","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"64GV3Y5J","preprint","2022","Martinet, Guillaume; Strzalkowski, Alexander; Engelhardt, Barbara E.","Variance Minimization in the Wasserstein Space for Invariant Causal Prediction","","","","","http://arxiv.org/abs/2110.07064","Selecting powerful predictors for an outcome is a cornerstone task for machine learning. However, some types of questions can only be answered by identifying the predictors that causally aﬀect the outcome. A recent approach to this causal inference problem leverages the invariance property of a causal mechanism across diﬀering experimental environments (Peters et al., 2016; Heinze-Deml et al., 2018). This method, invariant causal prediction (ICP), has a substantial computational defect – the runtime scales exponentially with the number of possible causal variables. In this work, we show that the approach taken in ICP may be reformulated as a series of nonparametric tests that scales linearly in the number of predictors. Each of these tests relies on the minimization of a novel loss function – the Wasserstein variance – that is derived from tools in optimal transport theory and is used to quantify distributional variability across environments. We prove under mild assumptions that our method is able to recover the set of identiﬁable direct causes, and we demonstrate in our experiments that it is competitive with other benchmark causal discovery algorithms.","2022-02-27","2022-05-17 21:30:35","2022-05-17 21:30:36","2022-05-17 21:30:35","","","","","","","","","","","","arXiv","","en","","","","","arXiv.org","","Number: arXiv:2110.07064 arXiv:2110.07064 [cs, stat]","","/home/rdyro/Zotero/storage/GQXF9DWI/Martinet et al. - 2022 - Variance Minimization in the Wasserstein Space for.pdf","","","Computer Science - Machine Learning; Statistics - Methodology","","","","","","","","","","","","","","","","","","","arXiv:2110.07064","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"96ASGDFH","preprint","2022","Sander, Ryan; Schwarting, Wilko; Seyde, Tim; Gilitschenski, Igor; Karaman, Sertac; Rus, Daniela","Neighborhood Mixup Experience Replay: Local Convex Interpolation for Improved Sample Efficiency in Continuous Control Tasks","","","","","http://arxiv.org/abs/2205.09117","Experience replay plays a crucial role in improving the sample efﬁciency of deep reinforcement learning agents. Recent advances in experience replay propose using Mixup (Zhang et al. 2018) to further improve sample efﬁciency via synthetic sample generation. We build upon this technique with Neighborhood Mixup Experience Replay (NMER), a geometrically-grounded replay buffer that interpolates transitions with their closest neighbors in state-action space. NMER preserves a locally linear approximation of the transition manifold by only applying Mixup between transitions with vicinal state-action features. Under NMER, a given transition’s set of state-action neighbors is dynamic and episode agnostic, in turn encouraging greater policy generalizability via inter-episode interpolation. We combine our approach with recent off-policy deep reinforcement learning algorithms and evaluate on continuous control environments. We observe that NMER improves sample efﬁciency by an average 94% (TD3) and 29% (SAC) over baseline replay buffers, enabling agents to effectively recombine previous experiences and learn from limited data.","2022-05-17","2022-05-23 23:01:56","2022-08-07 23:11:12","2022-05-23 23:01:56","","","","","","","Neighborhood Mixup Experience Replay","","","","","arXiv","","en","","","","","arXiv.org","","Number: arXiv:2205.09117 arXiv:2205.09117 [cs, eess]","","/home/rdyro/Zotero/storage/74K24HN9/Sander et al. - 2022 - Neighborhood Mixup Experience Replay Local Convex.pdf","","","Computer Science - Robotics; Computer Science - Machine Learning; Electrical Engineering and Systems Science - Systems and Control","","","","","","","","","","","","","","","","","","","arXiv:2205.09117","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"57MD2YTC","preprint","2022","Sahiner, Arda; Ergen, Tolga; Ozturkler, Batu; Pauly, John; Mardani, Morteza; Pilanci, Mert","Unraveling Attention via Convex Duality: Analysis and Interpretations of Vision Transformers","","","","","http://arxiv.org/abs/2205.08078","Vision transformers using self-attention or its proposed alternatives have demonstrated promising results in many image related tasks. However, the underpinning inductive bias of attention is not well understood. To address this issue, this paper analyzes attention through the lens of convex duality. For the non-linear dot-product selfattention, and alternative mechanisms such as MLP-mixer and Fourier Neural Operator (FNO), we derive equivalent ﬁnite-dimensional convex problems that are interpretable and solvable to global optimality. The convex programs lead to block nuclear-norm regularization that promotes low rank in the latent feature and token dimensions. In particular, we show how self-attention networks implicitly clusters the tokens, based on their latent similarity. We conduct experiments for transferring a pre-trained transformer backbone for CIFAR-100 classiﬁcation by ﬁne-tuning a variety of convex attention heads. The results indicate the merits of the bias induced by attention compared with the existing MLP or linear heads.","2022-05-20","2022-05-23 23:01:58","2022-08-07 23:16:31","2022-05-23 23:01:58","","","","","","","Unraveling Attention via Convex Duality","","","","","arXiv","","en","","","","","arXiv.org","","Number: arXiv:2205.08078 arXiv:2205.08078 [cs, math]","","/home/rdyro/Zotero/storage/CLNWWTLG/Sahiner et al. - 2022 - Unraveling Attention via Convex Duality Analysis .pdf","","","Computer Science - Computer Vision and Pattern Recognition; Computer Science - Machine Learning; Mathematics - Optimization and Control","","","","","","","","","","","","","","","","","","","arXiv:2205.08078","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"WRKN2N8R","preprint","2022","Laine, Forrest; Fridovich-Keil, David; Chiu, Chih-Yuan; Tomlin, Claire","The Computation of Approximate Generalized Feedback Nash Equilibria","","","","","http://arxiv.org/abs/2101.02900","We present the concept of a Generalized Feedback Nash Equilibrium (GFNE) in dynamic games, extending the Feedback Nash Equilibrium concept to games in which players are subject to state and input constraints. We formalize necessary and suﬃcient conditions for (local) GFNE solutions at the trajectory level, which enable the development of eﬃcient numerical methods for their computation. Speciﬁcally, we propose a Newton-style method for ﬁnding game trajectories which satisfy necessary conditions for an equilibrium, which can then be checked against suﬃciency conditions. We show that the evaluation of the necessary conditions in general requires computing a series of nested, implicitly-deﬁned derivatives, which quickly becomes intractable. To this end, we introduce an approximation to the necessary conditions which is amenable to eﬃcient evaluation, and in turn, computation of solutions. We term the solutions to the approximate necessary conditions Generalized Feedback Quasi-Nash Equilibria (GFQNE), and we introduce numerical methods for their computation. In particular, we develop a Sequential Linear-Quadratic Game approach, in which a LQ local approximation of the game is solved at each iteration. The development of this method relies on the ability to compute a GFNE to inequality- and equality-constrained LQ games, and therefore speciﬁc methods for the solution of these special cases are developed in detail. We demonstrate the eﬀectiveness of the proposed solution approach on a dynamic game arising in an autonomous driving application.","2022-03-18","2022-05-27 20:06:18","2022-05-27 20:06:19","2022-05-27 20:06:18","","","","","","","","","","","","arXiv","","en","","","","","arXiv.org","","Number: arXiv:2101.02900 arXiv:2101.02900 [cs, math]","","/home/rdyro/Zotero/storage/NKM6ZLXQ/Laine et al. - 2022 - The Computation of Approximate Generalized Feedbac.pdf","","","Computer Science - Robotics; Mathematics - Optimization and Control; Computer Science - Computer Science and Game Theory; Computer Science - Multiagent Systems","","","","","","","","","","","","","","","","","","","arXiv:2101.02900","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"3IJUCEMP","preprint","2022","Qiu, Zhaofan; Yao, Ting; Ngo, Chong-Wah; Mei, Tao","MLP-3D: A MLP-like 3D Architecture with Grouped Time Mixing","","","","10.48550/arXiv.2206.06292","http://arxiv.org/abs/2206.06292","Convolutional Neural Networks (CNNs) have been regarded as the go-to models for visual recognition. More recently, convolution-free networks, based on multi-head self-attention (MSA) or multi-layer perceptrons (MLPs), become more and more popular. Nevertheless, it is not trivial when utilizing these newly-minted networks for video recognition due to the large variations and complexities in video data. In this paper, we present MLP-3D networks, a novel MLP-like 3D architecture for video recognition. Specifically, the architecture consists of MLP-3D blocks, where each block contains one MLP applied across tokens (i.e., token-mixing MLP) and one MLP applied independently to each token (i.e., channel MLP). By deriving the novel grouped time mixing (GTM) operations, we equip the basic token-mixing MLP with the ability of temporal modeling. GTM divides the input tokens into several temporal groups and linearly maps the tokens in each group with the shared projection matrix. Furthermore, we devise several variants of GTM with different grouping strategies, and compose each variant in different blocks of MLP-3D network by greedy architecture search. Without the dependence on convolutions or attention mechanisms, our MLP-3D networks achieves 68.5\%/81.4\% top-1 accuracy on Something-Something V2 and Kinetics-400 datasets, respectively. Despite with fewer computations, the results are comparable to state-of-the-art widely-used 3D CNNs and video transformers. Source code is available at https://github.com/ZhaofanQiu/MLP-3D.","2022-06-13","2022-06-23 20:59:01","2022-06-23 20:59:01","2022-06-23 20:59:01","","","","","","","MLP-3D","","","","","arXiv","","","","","","","arXiv.org","","Number: arXiv:2206.06292 arXiv:2206.06292 [cs]","","/home/rdyro/Zotero/storage/IPTMSBBK/Qiu et al. - 2022 - MLP-3D A MLP-like 3D Architecture with Grouped Ti.pdf; /home/rdyro/Zotero/storage/YVPI5FME/2206.html","","","Computer Science - Artificial Intelligence; Computer Science - Computer Vision and Pattern Recognition; Computer Science - Multimedia","","","","","","","","","","","","","","","","","","","arXiv:2206.06292","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"GBZ4QWAS","preprint","2022","Peng, Cheng; Myronenko, Andriy; Hatamizadeh, Ali; Nath, Vish; Siddiquee, Md Mahfuzur Rahman; He, Yufan; Xu, Daguang; Chellappa, Rama; Yang, Dong","HyperSegNAS: Bridging One-Shot Neural Architecture Search with 3D Medical Image Segmentation using HyperNet","","","","10.48550/arXiv.2112.10652","http://arxiv.org/abs/2112.10652","Semantic segmentation of 3D medical images is a challenging task due to the high variability of the shape and pattern of objects (such as organs or tumors). Given the recent success of deep learning in medical image segmentation, Neural Architecture Search (NAS) has been introduced to find high-performance 3D segmentation network architectures. However, because of the massive computational requirements of 3D data and the discrete optimization nature of architecture search, previous NAS methods require a long search time or necessary continuous relaxation, and commonly lead to sub-optimal network architectures. While one-shot NAS can potentially address these disadvantages, its application in the segmentation domain has not been well studied in the expansive multi-scale multi-path search space. To enable one-shot NAS for medical image segmentation, our method, named HyperSegNAS, introduces a HyperNet to assist super-net training by incorporating architecture topology information. Such a HyperNet can be removed once the super-net is trained and introduces no overhead during architecture search. We show that HyperSegNAS yields better performing and more intuitive architectures compared to the previous state-of-the-art (SOTA) segmentation networks; furthermore, it can quickly and accurately find good architecture candidates under different computing constraints. Our method is evaluated on public datasets from the Medical Segmentation Decathlon (MSD) challenge, and achieves SOTA performances.","2022-03-24","2022-06-23 20:58:59","2022-06-23 20:58:59","2022-06-23 20:58:59","","","","","","","HyperSegNAS","","","","","arXiv","","","","","","","arXiv.org","","Number: arXiv:2112.10652 arXiv:2112.10652 [cs, eess]","","/home/rdyro/Zotero/storage/QYJEB249/2112.html","","","Computer Science - Computer Vision and Pattern Recognition; Electrical Engineering and Systems Science - Image and Video Processing","","","","","","","","","","","","","","","","","","","arXiv:2112.10652","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"V6CF3S9I","preprint","2022","Huang, Minbin; Huang, Zhijian; Li, Changlin; Chen, Xin; Xu, Hang; Li, Zhenguo; Liang, Xiaodan","Arch-Graph: Acyclic Architecture Relation Predictor for Task-Transferable Neural Architecture Search","","","","","http://arxiv.org/abs/2204.05941","Neural Architecture Search (NAS) aims to find efficient models for multiple tasks. Beyond seeking solutions for a single task, there are surging interests in transferring network design knowledge across multiple tasks. In this line of research, effectively modeling task correlations is vital yet highly neglected. Therefore, we propose \textbf{Arch-Graph}, a transferable NAS method that predicts task-specific optimal architectures with respect to given task embeddings. It leverages correlations across multiple tasks by using their embeddings as a part of the predictor's input for fast adaptation. We also formulate NAS as an architecture relation graph prediction problem, with the relational graph constructed by treating candidate architectures as nodes and their pairwise relations as edges. To enforce some basic properties such as acyclicity in the relational graph, we add additional constraints to the optimization process, converting NAS into the problem of finding a Maximal Weighted Acyclic Subgraph (MWAS). Our algorithm then strives to eliminate cycles and only establish edges in the graph if the rank results can be trusted. Through MWAS, Arch-Graph can effectively rank candidate models for each task with only a small budget to finetune the predictor. With extensive experiments on TransNAS-Bench-101, we show Arch-Graph's transferability and high sample efficiency across numerous tasks, beating many NAS methods designed for both single-task and multi-task search. It is able to find top 0.16\% and 0.29\% architectures on average on two search spaces under the budget of only 50 models.","2022-04-12","2022-06-23 20:58:29","2022-06-23 20:58:29","2022-06-23 20:58:29","","","","","","","Arch-Graph","","","","","arXiv","","","","","","","arXiv.org","","Number: arXiv:2204.05941 arXiv:2204.05941 [cs]","","/home/rdyro/Zotero/storage/7FUH3JBF/Huang et al. - 2022 - Arch-Graph Acyclic Architecture Relation Predicto.pdf; /home/rdyro/Zotero/storage/FJFWA3TY/2204.html","","","Computer Science - Computer Vision and Pattern Recognition; Computer Science - Machine Learning","","","","","","","","","","","","","","","","","","","arXiv:2204.05941","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"AXD6759A","preprint","2022","Qu, Liangqiong; Zhou, Yuyin; Liang, Paul Pu; Xia, Yingda; Wang, Feifei; Adeli, Ehsan; Fei-Fei, Li; Rubin, Daniel","Rethinking Architecture Design for Tackling Data Heterogeneity in Federated Learning","","","","10.48550/arXiv.2106.06047","http://arxiv.org/abs/2106.06047","Federated learning is an emerging research paradigm enabling collaborative training of machine learning models among different organizations while keeping data private at each institution. Despite recent progress, there remain fundamental challenges such as the lack of convergence and the potential for catastrophic forgetting across real-world heterogeneous devices. In this paper, we demonstrate that self-attention-based architectures (e.g., Transformers) are more robust to distribution shifts and hence improve federated learning over heterogeneous data. Concretely, we conduct the first rigorous empirical investigation of different neural architectures across a range of federated algorithms, real-world benchmarks, and heterogeneous data splits. Our experiments show that simply replacing convolutional networks with Transformers can greatly reduce catastrophic forgetting of previous devices, accelerate convergence, and reach a better global model, especially when dealing with heterogeneous data. We release our code and pretrained models at https://github.com/Liangqiong/ViT-FL-main to encourage future exploration in robust architectures as an alternative to current research efforts on the optimization front.","2022-04-13","2022-06-23 20:58:22","2022-06-23 20:58:22","2022-06-23 20:58:21","","","","","","","","","","","","arXiv","","","","","","","arXiv.org","","Number: arXiv:2106.06047 arXiv:2106.06047 [cs]","","/home/rdyro/Zotero/storage/VYXZR25A/Qu et al. - 2022 - Rethinking Architecture Design for Tackling Data H.pdf; /home/rdyro/Zotero/storage/AATYMNI2/2106.html","","","Computer Science - Computer Vision and Pattern Recognition; Computer Science - Machine Learning","","","","","","","","","","","","","","","","","","","arXiv:2106.06047","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"XLJU5XND","preprint","2022","Zhou, Qinqin; Sheng, Kekai; Zheng, Xiawu; Li, Ke; Sun, Xing; Tian, Yonghong; Chen, Jie; Ji, Rongrong","Training-free Transformer Architecture Search","","","","10.48550/arXiv.2203.12217","http://arxiv.org/abs/2203.12217","Recently, Vision Transformer (ViT) has achieved remarkable success in several computer vision tasks. The progresses are highly relevant to the architecture design, then it is worthwhile to propose Transformer Architecture Search (TAS) to search for better ViTs automatically. However, current TAS methods are time-consuming and existing zero-cost proxies in CNN do not generalize well to the ViT search space according to our experimental observations. In this paper, for the first time, we investigate how to conduct TAS in a training-free manner and devise an effective training-free TAS (TF-TAS) scheme. Firstly, we observe that the properties of multi-head self-attention (MSA) and multi-layer perceptron (MLP) in ViTs are quite different and that the synaptic diversity of MSA affects the performance notably. Secondly, based on the observation, we devise a modular strategy in TF-TAS that evaluates and ranks ViT architectures from two theoretical perspectives: synaptic diversity and synaptic saliency, termed as DSS-indicator. With DSS-indicator, evaluation results are strongly correlated with the test accuracies of ViT models. Experimental results demonstrate that our TF-TAS achieves a competitive performance against the state-of-the-art manually or automatically design ViT architectures, and it promotes the searching efficiency in ViT search space greatly: from about $24$ GPU days to less than $0.5$ GPU days. Moreover, the proposed DSS-indicator outperforms the existing cutting-edge zero-cost approaches (e.g., TE-score and NASWOT).","2022-03-23","2022-06-23 20:58:10","2022-06-23 20:58:10","2022-06-23 20:58:10","","","","","","","","","","","","arXiv","","","","","","","arXiv.org","","Number: arXiv:2203.12217 arXiv:2203.12217 [cs]","","/home/rdyro/Zotero/storage/6RF524PP/Zhou et al. - 2022 - Training-free Transformer Architecture Search.pdf; /home/rdyro/Zotero/storage/7XJ78XXL/2203.html","","","Computer Science - Computer Vision and Pattern Recognition","","","","","","","","","","","","","","","","","","","arXiv:2203.12217","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"EFIKRWCU","preprint","2022","Raychaudhuri, Dripta S.; Suh, Yumin; Schulter, Samuel; Yu, Xiang; Faraki, Masoud; Roy-Chowdhury, Amit K.; Chandraker, Manmohan","Controllable Dynamic Multi-Task Architectures","","","","10.48550/arXiv.2203.14949","http://arxiv.org/abs/2203.14949","Multi-task learning commonly encounters competition for resources among tasks, specifically when model capacity is limited. This challenge motivates models which allow control over the relative importance of tasks and total compute cost during inference time. In this work, we propose such a controllable multi-task network that dynamically adjusts its architecture and weights to match the desired task preference as well as the resource constraints. In contrast to the existing dynamic multi-task approaches that adjust only the weights within a fixed architecture, our approach affords the flexibility to dynamically control the total computational cost and match the user-preferred task importance better. We propose a disentangled training of two hypernetworks, by exploiting task affinity and a novel branching regularized loss, to take input preferences and accordingly predict tree-structured models with adapted weights. Experiments on three multi-task benchmarks, namely PASCAL-Context, NYU-v2, and CIFAR-100, show the efficacy of our approach. Project page is available at https://www.nec-labs.com/~mas/DYMU.","2022-03-28","2022-06-23 20:57:43","2022-06-23 20:57:43","2022-06-23 20:57:43","","","","","","","","","","","","arXiv","","","","","","","arXiv.org","","Number: arXiv:2203.14949 arXiv:2203.14949 [cs]","","/home/rdyro/Zotero/storage/Y32SMWFT/Raychaudhuri et al. - 2022 - Controllable Dynamic Multi-Task Architectures.pdf; /home/rdyro/Zotero/storage/CBHVKS7B/2203.html","","","Computer Science - Computer Vision and Pattern Recognition; Computer Science - Machine Learning","","","","","","","","","","","","","","","","","","","arXiv:2203.14949","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"UVA22WVU","preprint","2022","Xiao, Han; Wang, Ziwei; Zhu, Zheng; Zhou, Jie; Lu, Jiwen","Shapley-NAS: Discovering Operation Contribution for Neural Architecture Search","","","","10.48550/arXiv.2206.09811","http://arxiv.org/abs/2206.09811","In this paper, we propose a Shapley value based method to evaluate operation contribution (Shapley-NAS) for neural architecture search. Differentiable architecture search (DARTS) acquires the optimal architectures by optimizing the architecture parameters with gradient descent, which significantly reduces the search cost. However, the magnitude of architecture parameters updated by gradient descent fails to reveal the actual operation importance to the task performance and therefore harms the effectiveness of obtained architectures. By contrast, we propose to evaluate the direct influence of operations on validation accuracy. To deal with the complex relationships between supernet components, we leverage Shapley value to quantify their marginal contributions by considering all possible combinations. Specifically, we iteratively optimize the supernet weights and update the architecture parameters by evaluating operation contributions via Shapley value, so that the optimal architectures are derived by selecting the operations that contribute significantly to the tasks. Since the exact computation of Shapley value is NP-hard, the Monte-Carlo sampling based algorithm with early truncation is employed for efficient approximation, and the momentum update mechanism is adopted to alleviate fluctuation of the sampling process. Extensive experiments on various datasets and various search spaces show that our Shapley-NAS outperforms the state-of-the-art methods by a considerable margin with light search cost. The code is available at https://github.com/Euphoria16/Shapley-NAS.git","2022-06-20","2022-06-23 20:57:31","2022-06-23 20:57:31","2022-06-23 20:57:31","","","","","","","Shapley-NAS","","","","","arXiv","","","","","","","arXiv.org","","Number: arXiv:2206.09811 arXiv:2206.09811 [cs]","","/home/rdyro/Zotero/storage/6C6652C8/Xiao et al. - 2022 - Shapley-NAS Discovering Operation Contribution fo.pdf; /home/rdyro/Zotero/storage/IPQ8L9R4/2206.html","","","Computer Science - Computer Vision and Pattern Recognition; Computer Science - Machine Learning","","","","","","","","","","","","","","","","","","","arXiv:2206.09811","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"TTEFKDJE","preprint","2022","Wang, Yihan; Li, Muyang; Cai, Han; Chen, Wei-Ming; Han, Song","Lite Pose: Efficient Architecture Design for 2D Human Pose Estimation","","","","10.48550/arXiv.2205.01271","http://arxiv.org/abs/2205.01271","Pose estimation plays a critical role in human-centered vision applications. However, it is difficult to deploy state-of-the-art HRNet-based pose estimation models on resource-constrained edge devices due to the high computational cost (more than 150 GMACs per frame). In this paper, we study efficient architecture design for real-time multi-person pose estimation on edge. We reveal that HRNet's high-resolution branches are redundant for models at the low-computation region via our gradual shrinking experiments. Removing them improves both efficiency and performance. Inspired by this finding, we design LitePose, an efficient single-branch architecture for pose estimation, and introduce two simple approaches to enhance the capacity of LitePose, including Fusion Deconv Head and Large Kernel Convs. Fusion Deconv Head removes the redundancy in high-resolution branches, allowing scale-aware feature fusion with low overhead. Large Kernel Convs significantly improve the model's capacity and receptive field while maintaining a low computational cost. With only 25% computation increment, 7x7 kernels achieve +14.0 mAP better than 3x3 kernels on the CrowdPose dataset. On mobile platforms, LitePose reduces the latency by up to 5.0x without sacrificing performance, compared with prior state-of-the-art efficient pose estimation models, pushing the frontier of real-time multi-person pose estimation on edge. Our code and pre-trained models are released at https://github.com/mit-han-lab/litepose.","2022-05-20","2022-06-23 20:57:28","2022-06-23 20:57:28","2022-06-23 20:57:28","","","","","","","Lite Pose","","","","","arXiv","","","","","","","arXiv.org","","Number: arXiv:2205.01271 arXiv:2205.01271 [cs]","","/home/rdyro/Zotero/storage/LL4G65JE/Wang et al. - 2022 - Lite Pose Efficient Architecture Design for 2D Hu.pdf; /home/rdyro/Zotero/storage/VGEDBS3J/2205.html","","","Computer Science - Computer Vision and Pattern Recognition","","","","","","","","","","","","","","","","","","","arXiv:2205.01271","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"QJC4AYVC","preprint","2022","Wang, Haoxiang; Wang, Yite; Sun, Ruoyu; Li, Bo","Global Convergence of MAML and Theory-Inspired Neural Architecture Search for Few-Shot Learning","","","","10.48550/arXiv.2203.09137","http://arxiv.org/abs/2203.09137","Model-agnostic meta-learning (MAML) and its variants have become popular approaches for few-shot learning. However, due to the non-convexity of deep neural nets (DNNs) and the bi-level formulation of MAML, the theoretical properties of MAML with DNNs remain largely unknown. In this paper, we first prove that MAML with over-parameterized DNNs is guaranteed to converge to global optima at a linear rate. Our convergence analysis indicates that MAML with over-parameterized DNNs is equivalent to kernel regression with a novel class of kernels, which we name as Meta Neural Tangent Kernels (MetaNTK). Then, we propose MetaNTK-NAS, a new training-free neural architecture search (NAS) method for few-shot learning that uses MetaNTK to rank and select architectures. Empirically, we compare our MetaNTK-NAS with previous NAS methods on two popular few-shot learning benchmarks, miniImageNet, and tieredImageNet. We show that the performance of MetaNTK-NAS is comparable or better than the state-of-the-art NAS method designed for few-shot learning while enjoying more than 100x speedup. We believe the efficiency of MetaNTK-NAS makes itself more practical for many real-world tasks.","2022-03-17","2022-06-23 20:57:18","2022-06-23 20:57:18","2022-06-23 20:57:18","","","","","","","","","","","","arXiv","","","","","","","arXiv.org","","Number: arXiv:2203.09137 arXiv:2203.09137 [cs, stat]","","/home/rdyro/Zotero/storage/3IIJRWJB/Wang et al. - 2022 - Global Convergence of MAML and Theory-Inspired Neu.pdf; /home/rdyro/Zotero/storage/JBCKJQK2/2203.html","","","Computer Science - Computer Vision and Pattern Recognition; Computer Science - Machine Learning; Statistics - Machine Learning","","","","","","","","","","","","","","","","","","","arXiv:2203.09137","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"YXBAMMQ4","journalArticle","2022","Ren, Pengzhen; Xiao, Yun; Chang, Xiaojun; Huang, Po-yao; Li, Zhihui; Chen, Xiaojiang; Wang, Xin","A Comprehensive Survey of Neural Architecture Search: Challenges and Solutions","ACM Computing Surveys","","0360-0300, 1557-7341","10.1145/3447582","https://dl.acm.org/doi/10.1145/3447582","Deep learning has made substantial breakthroughs in many fields due to its powerful automatic representation capabilities. It has been proven that neural architecture design is crucial to the feature representation of data and the final performance. However, the design of the neural architecture heavily relies on the researchers’ prior knowledge and experience. And due to the limitations of humans’ inherent knowledge, it is difficult for people to jump out of their original thinking paradigm and design an optimal model. Therefore, an intuitive idea would be to reduce human intervention as much as possible and let the algorithm automatically design the neural architecture.                                Neural Architecture Search                              (               NAS               ) is just such a revolutionary algorithm, and the related research work is complicated and rich. Therefore, a comprehensive and systematic survey on the NAS is essential. Previously related surveys have begun to classify existing work mainly based on the key components of NAS: search space, search strategy, and evaluation strategy. While this classification method is more intuitive, it is difficult for readers to grasp the challenges and the landmark work involved. Therefore, in this survey, we provide a new perspective: beginning with an overview of the characteristics of the earliest NAS algorithms, summarizing the problems in these early NAS algorithms, and then providing solutions for subsequent related research work. In addition, we conduct a detailed and comprehensive analysis, comparison, and summary of these works. Finally, we provide some possible future research directions.","2022-05-31","2022-06-22 00:48:54","2022-06-22 00:48:54","2022-06-22 00:48:54","1-34","","4","54","","ACM Comput. Surv.","A Comprehensive Survey of Neural Architecture Search","","","","","","","en","","","","","DOI.org (Crossref)","","","","/home/rdyro/Zotero/storage/TRU9XVIS/Ren et al. - 2022 - A Comprehensive Survey of Neural Architecture Sear.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"JBPJUW2I","preprint","2022","Kapoor, Sayash; Narayanan, Arvind","Leakage and the Reproducibility Crisis in ML-based Science","","","","","http://arxiv.org/abs/2207.07048","The use of machine learning (ML) methods for prediction and forecasting has become widespread across the quantitative sciences. However, there are many known methodological pitfalls, including data leakage, in ML-based science. In this paper, we systematically investigate reproducibility issues in ML-based science. We show that data leakage is indeed a widespread problem and has led to severe reproducibility failures. Speciﬁcally, through a survey of literature in research communities that adopted ML methods, we ﬁnd 17 ﬁelds where errors have been found, collectively affecting 329 papers and in some cases leading to wildly overoptimistic conclusions. Based on our survey, we present a ﬁnegrained taxonomy of 8 types of leakage that range from textbook errors to open research problems.","2022-07-14","2022-07-15 22:17:06","2022-07-15 22:17:06","2022-07-15 22:17:06","","","","","","","","","","","","arXiv","","en","","","","","arXiv.org","","arXiv:2207.07048 [cs, stat]","","/home/rdyro/Zotero/storage/Y38RLIBI/Kapoor and Narayanan - 2022 - Leakage and the Reproducibility Crisis in ML-based.pdf","","","Computer Science - Artificial Intelligence; Computer Science - Machine Learning; Statistics - Methodology","","","","","","","","","","","","","","","","","","","arXiv:2207.07048","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"LGX7ITUF","journalArticle","2022","Yuan, Zi-Yang; Zhang, Lu; Wang, Hongxia; Zhang, Hui","Adaptively sketched Bregman projection methods for linear systems","Inverse Problems","","0266-5611, 1361-6420","10.1088/1361-6420/ac5f76","https://iopscience.iop.org/article/10.1088/1361-6420/ac5f76","The sketch-and-project, as a general archetypal algorithm for solving linear systems, unifies a variety of randomized iterative methods such as the randomized Kaczmarz and randomized coordinate descent. However, since it aims to find a least-norm solution from a linear system, the randomized sparse Kaczmarz can not be included. This motivates us to propose a more general framework, called sketched Bregman projection (SBP) method, in which we are able to find solutions with certain structures from linear systems. To generalize the concept of adaptive sampling to the SBP method, we show how the progress, measured by Bregman distance, of single step depends directly on a sketched loss function. Theoretically, we provide detailed global convergence results for the SBP method with different adaptive sampling rules. At last, for the (sparse) Kaczmarz methods, a group of numerical simulations are tested, with which we verify that the methods utilizing sampling Kaczmarz–Motzkin rule demands the fewest computational costs to achieve a given error bound comparing to the corresponding methods with other sampling rules.","2022-06-01","2022-10-07 21:03:46","2022-10-07 21:03:46","2022-10-07 21:03:46","065005","","6","38","","Inverse Problems","","","","","","","","en","","","","","DOI.org (Crossref)","","","","/home/rdyro/Zotero/storage/PHCNPYFW/Yuan et al. - 2022 - Adaptively sketched Bregman projection methods for.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"VJ4K7QK9","journalArticle","2022","Neu, Gergely; Lugosi, Gábor","Generalization Bounds via Convex Analysis","arXiv preprint arXiv:2202.04985","","","","","","2022","2022-11-01 23:25:01","2022-11-01 23:25:01","","","","","","","","","","","","","","","","","","","","Google Scholar","","","","/home/rdyro/Zotero/storage/RSAVFE5L/Neu and Lugosi - 2022 - Generalization Bounds via Convex Analysis.pdf; /home/rdyro/Zotero/storage/KJZUEFJU/2202.html","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"XHSRWLY8","journalArticle","2022","Vardi, Gal; Yehudai, Gilad; Shamir, Ohad","Width is Less Important than Depth in ReLU Neural Networks","arXiv preprint arXiv:2202.03841","","","","","","2022","2022-11-01 23:24:41","2022-11-01 23:24:41","","","","","","","","","","","","","","","","","","","","Google Scholar","","","","/home/rdyro/Zotero/storage/8L2ZV9HR/Vardi et al. - 2022 - Width is Less Important than Depth in ReLU Neural .pdf; /home/rdyro/Zotero/storage/TI4BZJQK/2202.html","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"QMWLGKYL","journalArticle","2022","Telgarsky, Matus","Stochastic linear optimization never overfits with quadratically-bounded losses on general data","arXiv preprint arXiv:2202.06915","","","","","","2022","2022-11-01 23:22:48","2022-12-16 14:22:36","","","","","","","","","","","","","","","","","","","","Google Scholar","","","","/home/rdyro/Zotero/storage/RGXPPYFY/Telgarsky - 2022 - Stochastic linear optimization never overfits with.pdf; /home/rdyro/Zotero/storage/ST2GR2TD/2202.html","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"EEP9GDVM","journalArticle","2022","Weissmann, Simon; Wilson, Ashia; Zech, Jakob","Multilevel Optimization for Inverse Problems","arXiv preprint arXiv:2204.13732","","","","","","2022","2022-11-01 23:22:40","2022-12-16 14:23:00","","","","","","","","","","","","","","","","","","","","Google Scholar","","","","/home/rdyro/Zotero/storage/VEDXV97P/Weissmann et al. - 2022 - Multilevel Optimization for Inverse Problems.pdf; /home/rdyro/Zotero/storage/QKX4IKGU/2204.html","","","65N21, 65N75, 65K10; Mathematics - Numerical Analysis; Mathematics - Optimization and Control","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"4CZUH8JU","conferencePaper","2022","Li, Chris Junchi; Mou, Wenlong; Wainwright, Martin; Jordan, Michael","Root-sgd: Sharp nonasymptotics and asymptotic efficiency in a single algorithm","Conference on Learning Theory","","","","","","2022","2022-11-01 23:22:24","2022-12-16 14:22:31","","909–981","","","","","","Root-sgd","","","","","PMLR","","","","","","","Google Scholar","","","","/home/rdyro/Zotero/storage/VJXZRJBU/Li et al. - 2022 - Root-sgd Sharp nonasymptotics and asymptotic effi.pdf; /home/rdyro/Zotero/storage/N2HUZDN8/li22a.html","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"DYDDRIB8","conferencePaper","2022","Kelner, Jonathan; Marsden, Annie; Sharan, Vatsal; Sidford, Aaron; Valiant, Gregory; Yuan, Honglin","Big-Step-Little-Step: Efficient Gradient Methods for Objectives with Multiple Scales","Conference on Learning Theory","","","","","","2022","2022-11-01 23:22:13","2022-12-16 14:23:10","","2431–2540","","","","","","Big-Step-Little-Step","","","","","PMLR","","","","","","","Google Scholar","","","","/home/rdyro/Zotero/storage/RKY8DWZ3/Kelner et al. - 2022 - Big-Step-Little-Step Efficient Gradient Methods f.pdf; /home/rdyro/Zotero/storage/P8YVFUNK/kelner22a.html","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"PZYRYZCM","preprint","2022","Singh, Dinesh; Tankaria, Hardik; Yamada, Makoto","Nys-Newton: Nystr\""om-Approximated Curvature for Stochastic Optimization","","","","10.48550/arXiv.2110.08577","http://arxiv.org/abs/2110.08577","Second-order optimization methods are among the most widely used optimization approaches for convex optimization problems, and have recently been used to optimize non-convex optimization problems such as deep learning models. The widely used second-order optimization methods such as quasi-Newton methods generally provide curvature information by approximating the Hessian using the secant equation. However, the secant equation becomes insipid in approximating the Newton step owing to its use of the first-order derivatives. In this study, we propose an approximate Newton sketch-based stochastic optimization algorithm for large-scale empirical risk minimization. Specifically, we compute a partial column Hessian of size ($d\times m$) with $m\ll d$ randomly selected variables, then use the \emph{Nystr\""om method} to better approximate the full Hessian matrix. To further reduce the computational complexity per iteration, we directly compute the update step ($\Delta\boldsymbol{w}$) without computing and storing the full Hessian or its inverse. We then integrate our approximated Hessian with stochastic gradient descent and stochastic variance-reduced gradient methods. The results of numerical experiments on both convex and non-convex functions show that the proposed approach was able to obtain a better approximation of Newton\textquotesingle s method, exhibiting performance competitive with that of state-of-the-art first-order and stochastic quasi-Newton methods. Furthermore, we provide a theoretical convergence analysis for convex functions.","2022-01-29","2022-11-16 00:15:37","2022-12-16 14:23:05","2022-11-16 00:15:37","","","","","","","Nys-Newton","","","","","arXiv","","","","","","","arXiv.org","","arXiv:2110.08577 [cs, math, stat]","","/home/rdyro/Zotero/storage/5RPVTX4C/Singh et al. - 2022 - Nys-Newton Nystrom-Approximated Curvature for S.pdf; /home/rdyro/Zotero/storage/LABXA4EX/2110.html","","","Computer Science - Machine Learning; Mathematics - Optimization and Control; Statistics - Machine Learning","","","","","","","","","","","","","","","","","","","arXiv:2110.08577","","","","","","","","","","","","","","","","","","","","","","","","","","",""